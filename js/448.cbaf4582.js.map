{"version":3,"file":"js/448.cbaf4582.js","mappings":"8RAOO,MAAMA,EACXC,WAAAA,GACEC,KAAKC,iBAAmB,mBACxBD,KAAKE,aAAc,EACnBF,KAAKG,UAAY,IACnB,CAKA,gBAAMC,GACJ,IAKE,OAJAC,QAAQC,IAAI,4BACZN,KAAKG,WAAYI,EAAAA,EAAAA,YAAW,IAAIC,EAAAA,SAChCR,KAAKE,aAAc,EACnBG,QAAQC,IAAI,2BACL,CACT,CAAE,MAAOG,GAIP,OAHAJ,QAAQI,MAAM,0BAA2BA,GAEzCT,KAAKE,aAAc,GACZ,CACT,CACF,CAOA,cAAMQ,CAASC,GACb,IAAKA,EAAM,MAAO,GAElB,IAAIX,KAAKG,YAAaH,KAAKE,YAezB,OAAOF,KAAKY,iBAAiBD,GAd7B,IAEE,MAAME,EAASb,KAAKG,UAAUW,UAAUH,GAGxC,OAFAN,QAAQC,IAAI,mBAAoBO,GAChCR,QAAQC,IAAI,aAAcS,KAAKC,UAAUH,EAAQ,KAAM,IAChDA,EACJI,IAAKC,GAASA,EAAKC,GACnBC,OAAQC,GAASA,EAAKC,OAAOC,OAAS,GACtCH,OAAQC,GAASrB,KAAKwB,YAAYH,GACvC,CAAE,MAAOZ,GAEP,OADAJ,QAAQI,MAAM,2BAA4BA,GACnCT,KAAKY,iBAAiBD,EAC/B,CAIJ,CAOAa,WAAAA,CAAYH,GACV,IAAKA,GAA+B,IAAvBA,EAAKC,OAAOC,OAAc,OAAO,EAE9C,MAAME,EAAUJ,EAAKC,OAGrB,QAAK,qBAAqBI,KAAKD,MAG3B,mBAAmBC,KAAKD,KAGxB,wBAAwBC,KAAKD,GAGnC,CAOAb,gBAAAA,CAAiBD,GACf,IAAKA,EAAM,MAAO,GAElB,MAAMgB,EAAa3B,KAAK4B,UAAUjB,GAC5BkB,EAAeF,EAAWG,MAAM9B,KAAKC,mBAAqB,GAG1D8B,EAAS,GACf,IAAIC,EAAI,EAER,MAAOA,EAAIH,EAAaN,OAAQ,CAC9B,IAAIU,GAAQ,EAGZ,IAAK,IAAIC,EAAMC,KAAKC,IAAI,EAAGP,EAAaN,OAASS,GAAIE,GAAO,EAAGA,IAAO,CACpE,MAAMG,EAAYR,EAAaS,MAAMN,EAAGA,EAAIE,GAAKK,KAAK,IAGtD,GAAIvC,KAAKwC,YAAYC,IAAIJ,GAAY,CACnCN,EAAOW,KAAKL,GACZL,GAAKE,EACLD,GAAQ,EACR,KACF,CACF,CAGA,IAAKA,EAAO,CACV,MAAMZ,EAAOQ,EAAaS,MAAMN,EAAGA,EAAI,GAAGO,KAAK,IAC/CR,EAAOW,KAAKrB,GACZW,GAAK,CACP,CACF,CAEA,OAAOD,EAAOX,OAAQC,GAASA,EAAKC,OAAOC,OAAS,EACtD,CAOAoB,cAAAA,CAAehC,GACb,IAAKA,EAAM,MAAO,GAClB,MAAMkB,EAAelB,EAAKmB,MAAM9B,KAAKC,mBAAqB,GAC1D,OAAO4B,CACT,CAOA,oBAAMe,CAAejC,GACnB,aAAaX,KAAKU,SAASC,EAC7B,CAQA,yBAAMkC,CAAoBC,EAAOC,GAC/B,MAAMC,QAAgBhD,KAAKU,SAASoC,GAC9BG,QAAgBjD,KAAKU,SAASqC,GAEpC,GAAuB,IAAnBC,EAAQzB,QAAmC,IAAnB0B,EAAQ1B,OAAc,OAAO,EACzD,GAAuB,IAAnByB,EAAQzB,QAAmC,IAAnB0B,EAAQ1B,OAAc,OAAO,EAEzD,MAAM2B,EAAO,IAAIC,IAAIH,GACfI,EAAO,IAAID,IAAIF,GAEfI,EAAe,IAAIF,IAAI,IAAID,GAAM9B,OAAQkC,GAAMF,EAAKX,IAAIa,KACxDC,EAAQ,IAAIJ,IAAI,IAAID,KAASE,IAEnC,OAAOC,EAAaG,KAAOD,EAAMC,IACnC,CAOA,qBAAMC,CAAgB9C,GACpB,IAAKA,EAAM,MAAO,GAElB,IACE,MAAM+C,EAAeC,EAAOhD,EAAM,CAChCiD,MAAOD,EAAAA,WACPE,WAAW,IAEb,OAAOH,EAAaI,OAAOvB,KAAK,IAClC,CAAE,MAAO9B,GAEP,OADAJ,QAAQI,MAAM,YAAaA,GACpBT,KAAK+D,eAAepD,EAC7B,CACF,CAOAoD,cAAAA,CAAepD,GACb,MAAMqD,EAAY,CAChBC,IAAG,KACHC,IAAG,KACHC,IAAG,KACHC,IAAG,KACHC,IAAG,KACHC,KAAI,QACJC,KAAI,QACJC,KAAI,QACJC,IAAG,MACHC,IAAG,OACHC,IAAG,KACHC,IAAG,OACHC,KAAI,QACJC,KAAI,UACJC,KAAI,SACJC,KAAI,UACJC,KAAI,WACJC,KAAI,UACJC,KAAI,WACJC,KAAI,SACJC,KAAI,QACJC,KAAI,UACJC,KAAI,WACJC,KAAI,YACJC,KAAI,SACJC,KAAI,WACJC,KAAI,SACJC,KAAI,YAGN,OAAOjF,EACJkF,MAAM,IACN5E,IAAK6E,GAAS9B,EAAU8B,IAASA,GACjCvD,KAAK,IACV,CAOAX,SAAAA,CAAUjB,GACR,OAAKA,EAEEA,EACJoF,QAAQ,mDAAoD,KAC5DA,QAAQ,OAAQ,KAChBzE,OALe,EAMpB,CAQA,wBAAM0E,CAAmBC,EAAcC,GACrC,MAAMC,QAAuBnG,KAAKU,SAASuF,GACrCG,QAAsBpG,KAAKU,SAASwF,GAEpCG,EAAc,IAAIlD,IAAIgD,GACtBG,EAAa,IAAInD,IAAIiD,GAErBG,EAAYJ,EAAe/E,OAAQoF,IAAWF,EAAW7D,IAAI+D,IAC7DC,EAAYL,EAAchF,OAAQoF,IAAWH,EAAY5D,IAAI+D,IAEnE,MAAO,CACLD,YACAE,YACAC,iBAAkB1G,KAAK6C,oBAAoBoD,EAAcC,GAE7D,CAMAS,aAAAA,GACE,OAAO3G,KAAKE,WACd,E","sources":["webpack://dkvoice-web-vue/./node_modules/@daekyo/dkvoiceweb/src/SegmentitTokenizer.js"],"sourcesContent":["/**\n * segmentit 라이브러리를 사용한 중국어 토크나이저\n * https://github.com/linonetwo/segmentit\n */\nimport pinyin from \"pinyin\";\nimport { Segment, useDefault } from \"segmentit\";\n\nexport class SegmentitTokenizer {\n  constructor() {\n    this.chineseCharRegex = /[\\u4e00-\\u9fff]/g;\n    this.initialized = false;\n    this.segmentit = null;\n  }\n\n  /**\n   * segmentit 토크나이저 초기화\n   */\n  async initialize() {\n    try {\n      console.log(\"segmentit 토크나이저 초기화 중...\");\n      this.segmentit = useDefault(new Segment());\n      this.initialized = true;\n      console.log(\"segmentit 토크나이저 초기화 완료\");\n      return true;\n    } catch (error) {\n      console.error(\"segmentit 토크나이저 초기화 실패:\", error);\n      // 폴백: 기본 토크나이저 사용\n      this.initialized = true;\n      return false;\n    }\n  }\n\n  /**\n   * segmentit을 사용한 중국어 토큰화\n   * @param {string} text - 중국어 텍스트\n   * @returns {Promise<string[]>} 토큰 배열\n   */\n  async tokenize(text) {\n    if (!text) return [];\n\n    if (this.segmentit && this.initialized) {\n      try {\n        // segmentit 사용\n        const result = this.segmentit.doSegment(text);\n        console.log(\"segmentit 원본 결과:\", result);\n        console.log(\"result 구조:\", JSON.stringify(result, null, 2));\n        return result\n          .map((item) => item.w)\n          .filter((word) => word.trim().length > 0)\n          .filter((word) => this.isValidWord(word)); // 특수문자 제거\n      } catch (error) {\n        console.error(\"segmentit 토큰화 실패, 폴백 사용:\", error);\n        return this.fallbackTokenize(text);\n      }\n    } else {\n      return this.fallbackTokenize(text);\n    }\n  }\n\n  /**\n   * 단어가 유효한지 확인 (특수문자 제거)\n   * @param {string} word - 검사할 단어\n   * @returns {boolean} 유효한 단어 여부\n   */\n  isValidWord(word) {\n    if (!word || word.trim().length === 0) return false;\n\n    const trimmed = word.trim();\n\n    // 중국어 문자만 포함하는지 확인\n    if (!/^[\\u4e00-\\u9fff]+$/.test(trimmed)) return false;\n\n    // 특수문자가 포함되지 않았는지 확인\n    if (/[^\\u4e00-\\u9fff]/.test(trimmed)) return false;\n\n    // 중국어 문장부호가 포함되지 않았는지 확인\n    if (/[，。！？；：\"\"''（）【】《》、…—]/.test(trimmed)) return false;\n\n    return true;\n  }\n\n  /**\n   * 폴백 토큰화 (기존 알고리즘)\n   * @param {string} text - 중국어 텍스트\n   * @returns {string[]} 토큰 배열\n   */\n  fallbackTokenize(text) {\n    if (!text) return [];\n\n    const normalized = this.normalize(text);\n    const chineseChars = normalized.match(this.chineseCharRegex) || [];\n\n    // 사전 기반 토큰화\n    const tokens = [];\n    let i = 0;\n\n    while (i < chineseChars.length) {\n      let found = false;\n\n      // 4글자부터 1글자까지 역순으로 검색\n      for (let len = Math.min(4, chineseChars.length - i); len >= 1; len--) {\n        const candidate = chineseChars.slice(i, i + len).join(\"\");\n\n        // 사전에 있는 단어인지 확인\n        if (this.commonWords.has(candidate)) {\n          tokens.push(candidate);\n          i += len;\n          found = true;\n          break;\n        }\n      }\n\n      // 사전에 없는 경우 2글자씩 묶어서 처리\n      if (!found) {\n        const word = chineseChars.slice(i, i + 2).join(\"\");\n        tokens.push(word);\n        i += 2;\n      }\n    }\n\n    return tokens.filter((word) => word.trim().length > 0);\n  }\n\n  /**\n   * 중국어 텍스트를 문자 단위로 토큰화\n   * @param {string} text - 중국어 텍스트\n   * @returns {string[]} 문자 단위 토큰 배열\n   */\n  tokenizeByChar(text) {\n    if (!text) return [];\n    const chineseChars = text.match(this.chineseCharRegex) || [];\n    return chineseChars;\n  }\n\n  /**\n   * 중국어 텍스트를 단어 단위로 토큰화\n   * @param {string} text - 중국어 텍스트\n   * @returns {Promise<string[]>} 단어 단위 토큰 배열\n   */\n  async tokenizeByWord(text) {\n    return await this.tokenize(text);\n  }\n\n  /**\n   * 두 중국어 텍스트의 유사도 계산\n   * @param {string} text1 - 첫 번째 텍스트\n   * @param {string} text2 - 두 번째 텍스트\n   * @returns {Promise<number>} 유사도 점수 (0~1)\n   */\n  async calculateSimilarity(text1, text2) {\n    const tokens1 = await this.tokenize(text1);\n    const tokens2 = await this.tokenize(text2);\n\n    if (tokens1.length === 0 && tokens2.length === 0) return 1.0;\n    if (tokens1.length === 0 || tokens2.length === 0) return 0.0;\n\n    const set1 = new Set(tokens1);\n    const set2 = new Set(tokens2);\n\n    const intersection = new Set([...set1].filter((x) => set2.has(x)));\n    const union = new Set([...set1, ...set2]);\n\n    return intersection.size / union.size;\n  }\n\n  /**\n   * 병음 변환\n   * @param {string} text - 중국어 텍스트\n   * @returns {Promise<string>} 병음 문자열\n   */\n  async convertToPinyin(text) {\n    if (!text) return \"\";\n\n    try {\n      const pinyinResult = pinyin(text, {\n        style: pinyin.STYLE_TONE,\n        heteronym: false,\n      });\n      return pinyinResult.flat().join(\" \");\n    } catch (error) {\n      console.error(\"병음 변환 실패:\", error);\n      return this.fallbackPinyin(text);\n    }\n  }\n\n  /**\n   * 폴백 병음 변환\n   * @param {string} text - 중국어 텍스트\n   * @returns {string} 병음 문자열\n   */\n  fallbackPinyin(text) {\n    const pinyinMap = {\n      我: \"wǒ\",\n      你: \"nǐ\",\n      他: \"tā\",\n      她: \"tā\",\n      它: \"tā\",\n      我们: \"wǒmen\",\n      你们: \"nǐmen\",\n      他们: \"tāmen\",\n      好: \"hǎo\",\n      坏: \"huài\",\n      大: \"dà\",\n      小: \"xiǎo\",\n      学习: \"xuéxí\",\n      工作: \"gōngzuò\",\n      吃饭: \"chīfàn\",\n      今天: \"jīntiān\",\n      明天: \"míngtiān\",\n      昨天: \"zuótiān\",\n      中国: \"zhōngguó\",\n      美国: \"měiguó\",\n      日本: \"rìběn\",\n      北京: \"běijīng\",\n      上海: \"shànghǎi\",\n      广州: \"guǎngzhōu\",\n      世界: \"shìjiè\",\n      学生: \"xuéshēng\",\n      喜欢: \"xǐhuān\",\n      中文: \"zhōngwén\",\n    };\n\n    return text\n      .split(\"\")\n      .map((char) => pinyinMap[char] || char)\n      .join(\" \");\n  }\n\n  /**\n   * 텍스트 정규화\n   * @param {string} text - 정규화할 텍스트\n   * @returns {string} 정규화된 텍스트\n   */\n  normalize(text) {\n    if (!text) return \"\";\n\n    return text\n      .replace(/[\\u3000\\u2000-\\u200f\\u2028-\\u202f\\u205f-\\u206f]/g, \" \") // 전각 공백을 반각으로\n      .replace(/\\s+/g, \" \") // 연속된 공백을 하나로\n      .trim();\n  }\n\n  /**\n   * 차이점 분석\n   * @param {string} originalText - 원문\n   * @param {string} compareText - 비교할 문장\n   * @returns {Promise<object>} 차이점 분석 결과\n   */\n  async analyzeDifferences(originalText, compareText) {\n    const originalTokens = await this.tokenize(originalText);\n    const compareTokens = await this.tokenize(compareText);\n\n    const originalSet = new Set(originalTokens);\n    const compareSet = new Set(compareTokens);\n\n    const omissions = originalTokens.filter((token) => !compareSet.has(token));\n    const additions = compareTokens.filter((token) => !originalSet.has(token));\n\n    return {\n      omissions,\n      additions,\n      similarity: await this.calculateSimilarity(originalText, compareText),\n    };\n  }\n\n  /**\n   * 초기화 상태 확인\n   * @returns {boolean} 초기화 여부\n   */\n  isInitialized() {\n    return this.initialized;\n  }\n}\n"],"names":["SegmentitTokenizer","constructor","this","chineseCharRegex","initialized","segmentit","initialize","console","log","useDefault","Segment","error","tokenize","text","fallbackTokenize","result","doSegment","JSON","stringify","map","item","w","filter","word","trim","length","isValidWord","trimmed","test","normalized","normalize","chineseChars","match","tokens","i","found","len","Math","min","candidate","slice","join","commonWords","has","push","tokenizeByChar","tokenizeByWord","calculateSimilarity","text1","text2","tokens1","tokens2","set1","Set","set2","intersection","x","union","size","convertToPinyin","pinyinResult","pinyin","style","heteronym","flat","fallbackPinyin","pinyinMap","我","你","他","她","它","我们","你们","他们","好","坏","大","小","学习","工作","吃饭","今天","明天","昨天","中国","美国","日本","北京","上海","广州","世界","学生","喜欢","中文","split","char","replace","analyzeDifferences","originalText","compareText","originalTokens","compareTokens","originalSet","compareSet","omissions","token","additions","similarity","isInitialized"],"sourceRoot":""}